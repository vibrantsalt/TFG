{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis automático de alteraciones en la escritura manuscrita debidas al Párkinson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import zipfile\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Para uso en Colab. La estructura de archivos esperada se explica en la memoria.\n",
    "Se recomienda ejecutar el proyecto localmente porque para entrenar la red neuronal convolucional y generar las predicciones se hace uso de la CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# Colab. Necesario para subir PaHaW.zip, los módulos y otros archivos.\n",
    "from google.colab import files\n",
    "files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# Colab. Se extrae PaHaW.zip\n",
    "try:\n",
    "    with zipfile.ZipFile(\"PaHaW.zip\") as zip_pahaw:\n",
    "        zip_pahaw.extractall()\n",
    "        print(\"Se ha extraído PaHaW.zip\")\n",
    "except:\n",
    "    print(\"Error al extraer PaHaW.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# Colab (opcional). Se extrae generated.zip\n",
    "try:\n",
    "    with zipfile.ZipFile(\"generated.zip\") as zip_generated:\n",
    "        zip_generated.extractall()\n",
    "        print(\"Se ha extraído generated.zip\")\n",
    "except:\n",
    "    print(\"Error al extraer generated.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pahaw_loader\n",
    "import nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Valores relevantes del proyecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta para cargar (si existe) o guardar el modelo entrenado.\n",
    "model_file_path = os.path.join(\"generated\", \"default.obj\")\n",
    "\n",
    "training_epochs = 30  # En la memoria se usó 30 en todos los casos.\n",
    "random_seed = 17  # En la memoria se usó 17 en todos los casos.\n",
    "task_number = 2  # El trabajo es compatible con las tareas 2, 3 y 4.\n",
    "clipping_side_size = 111  # Ha de ser impar y mayor de 71. Se recomienda 111.\n",
    "clipping_jump_size = 1  # Si es mayor de 1 hay puntos que pueden quedar sin predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero se cargan los sujetos y las tareas.\n",
    "subjects_pd_status_years, subjects_tasks = pahaw_loader.load()\n",
    "\n",
    "# Para las configuraciones 2, 3 y 4.\n",
    "worst_pd_id_list = [4, 7, 9, 17, 18, 19, 53, 55]\n",
    "best_h_id_list = [30, 49, 52, 67, 69, 70, 84, 87]\n",
    "\n",
    "# Para la configuración 4.\n",
    "outliers_false_negatives = [20, 24, 33, 43, 44, 74, 75, 78]\n",
    "outliers_false_positives = [28, 57, 83, 91]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selección de sujetos para entrenamiento y test de la red neuronal convolucional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "# Para la configuración 1. Se usan todos los sujetos PaHaW en base a su estado PD.\n",
    "h_id_list = []\n",
    "pd_id_list = []\n",
    "\n",
    "subjects_id_list = list(subjects_tasks.keys())\n",
    "\n",
    "for subject_id in subjects_id_list:\n",
    "    if subjects_pd_status_years[subject_id][0] == 0:\n",
    "        h_id_list.append(subject_id)\n",
    "    else:\n",
    "        pd_id_list.append(subject_id)\n",
    "\n",
    "random.Random(random_seed).shuffle(h_id_list)\n",
    "random.Random(random_seed).shuffle(pd_id_list)\n",
    "\n",
    "# Hay 75 sujetos en total, 62 para entrenamiento y 13 para test.\n",
    "test_id_list = h_id_list[:7]\n",
    "train_id_list = h_id_list[7:]\n",
    "test_id_list.extend(pd_id_list[:6])\n",
    "train_id_list.extend(pd_id_list[6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# Para la configuración 2. Se usan los 16 sujetos seleccionados según las\n",
    "# características PD visibles y todos los sujetos PaHaW en base a su estado PD.\n",
    "h_id_list = []\n",
    "pd_id_list = []\n",
    "\n",
    "subjects_id_list = list(subjects_tasks.keys())\n",
    "\n",
    "for subject_id in subjects_id_list:\n",
    "    if subjects_pd_status_years[subject_id][0] == 0:\n",
    "        h_id_list.append(subject_id)\n",
    "    else:\n",
    "        pd_id_list.append(subject_id)\n",
    "\n",
    "for subject_id in worst_pd_id_list:\n",
    "    pd_id_list.remove(subject_id)\n",
    "\n",
    "for subject_id in best_h_id_list:\n",
    "    h_id_list.remove(subject_id)\n",
    "\n",
    "random.Random(random_seed).shuffle(worst_pd_id_list)\n",
    "random.Random(random_seed).shuffle(best_h_id_list)\n",
    "random.Random(random_seed).shuffle(h_id_list)\n",
    "random.Random(random_seed).shuffle(pd_id_list)\n",
    "\n",
    "# Hay 75 sujetos en total, 62 para entrenamiento y 13 para test.\n",
    "test_id_list = worst_pd_id_list[:4]\n",
    "train_id_list = worst_pd_id_list[4:]\n",
    "test_id_list.extend(best_h_id_list[:4])\n",
    "train_id_list.extend(best_h_id_list[4:])\n",
    "test_id_list.extend(h_id_list[:3])\n",
    "train_id_list.extend(h_id_list[3:])\n",
    "test_id_list.extend(pd_id_list[:2])\n",
    "train_id_list.extend(pd_id_list[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# Para las configuraciones 3 y 4. Se usan únicamente los 16 sujetos seleccionados.\n",
    "random.Random(random_seed).shuffle(worst_pd_id_list)\n",
    "random.Random(random_seed).shuffle(best_h_id_list)\n",
    "\n",
    "# Hay 16 sujetos en total, 14 para entrenamiento y 2 para test.\n",
    "test_id_list = worst_pd_id_list[:1]\n",
    "train_id_list = worst_pd_id_list[1:]\n",
    "\n",
    "test_id_list.extend(best_h_id_list[:1])\n",
    "train_id_list.extend(best_h_id_list[1:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generación de los recortes para los grupos de entrenamiento y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora se generan todos los recortes y los diccionarios para entrenamiento y test.\n",
    "print(\n",
    "    f\"Generando los recortes para train ({len(train_id_list)}) y test ({len(test_id_list)}).\"\n",
    ")\n",
    "start_time = time.time()\n",
    "\n",
    "train_id_list.sort()\n",
    "test_id_list.sort()\n",
    "\n",
    "# Las imágenes de los recortes se guardan en un directorio cuyo nombre depende de la tarea, del tamaño\n",
    "# de salto y del tamaño de recorte. De esta manera pueden ser usadas en ejecuciones posteriores.\n",
    "subjects_id_list = train_id_list + test_id_list\n",
    "for subject_id in subjects_id_list:\n",
    "    for letters_set in subjects_tasks[subject_id][task_number].letters_sets_list:\n",
    "        letters_set.generate_clippings(clipping_side_size, clipping_jump_size)\n",
    "\n",
    "# Las imágenes de los recortes generados se copian a los directorios\n",
    "# de entrenamiento y test en base al grupo al que pertenece el sujeto.\n",
    "shutil.rmtree(os.path.join(\"generated\", \"test\"), ignore_errors=True)\n",
    "os.makedirs(os.path.join(\"generated\", \"test\"))\n",
    "shutil.rmtree(os.path.join(\"generated\", \"train\"), ignore_errors=True)\n",
    "os.makedirs(os.path.join(\"generated\", \"train\"))\n",
    "\n",
    "test_clippings = {}\n",
    "test_clippings_pd = {}\n",
    "train_clippings = {}\n",
    "train_clippings_pd = {}\n",
    "\n",
    "for subject_id in test_id_list:\n",
    "    for letters_set in subjects_tasks[subject_id][task_number].letters_sets_list:\n",
    "        for clipping in letters_set.clippings_list:\n",
    "            test_clippings[clipping.name] = clipping\n",
    "            test_clippings_pd[clipping.name] = subjects_pd_status_years[subject_id][0]\n",
    "            clipping.copy_clipping(\"test\")\n",
    "\n",
    "for subject_id in train_id_list:\n",
    "    for letters_set in subjects_tasks[subject_id][task_number].letters_sets_list:\n",
    "        for clipping in letters_set.clippings_list:\n",
    "            train_clippings[clipping.name] = clipping\n",
    "            train_clippings_pd[clipping.name] = subjects_pd_status_years[subject_id][0]\n",
    "            clipping.copy_clipping(\"train\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Recortes generados en {(end_time - start_time):.2f} segundos.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento de la red neuronal convolucional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aquí se entrena el modelo si es necesario.\n",
    "train_dataset = nn.ClippingsDataset(\"train\", train_clippings_pd, train_clippings)\n",
    "train_loader = nn.DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "test_dataset = nn.ClippingsDataset(\"test\", test_clippings_pd, test_clippings)\n",
    "test_loader = nn.DataLoader(test_dataset, batch_size=10, shuffle=False)\n",
    "\n",
    "# Si el modelo existe en disco se intenta cargar y se salta el entrenamiento.\n",
    "if os.path.exists(model_file_path):\n",
    "    file = open(model_file_path, \"rb\")\n",
    "    model = pickle.load(file)\n",
    "    file.close()\n",
    "    if model.check(\n",
    "        task_number,\n",
    "        clipping_side_size,\n",
    "        clipping_jump_size,\n",
    "        training_epochs,\n",
    "        train_id_list,\n",
    "    ):\n",
    "        # Si el modelo en la ruta es el esperado, se muestran los resultados del entrenamiento.\n",
    "        print(\"Modelo cargado con éxito:\")\n",
    "        for results_i in model.train_results:\n",
    "            if (results_i[\"epoch\"] + 1) % 5 == 0:\n",
    "                print(\n",
    "                    f\"Epoch: {results_i['epoch'] + 1}/{len(model.train_results)} | Trai\"\n",
    "                    f\"n loss: {results_i['train_loss']:.4f}, Train accuracy: \"\n",
    "                    f\"{(results_i['train_accuracy'] * 100):.2f} | Test loss: \"\n",
    "                    f\"{results_i['test_loss']:.4f}, Test accuracy: \"\n",
    "                    f\"{(results_i['test_accuracy'] * 100):.2f}\"\n",
    "                )\n",
    "\n",
    "    # Si el modelo no es el esperado, se para la ejecución y se muestra el error.\n",
    "    else:\n",
    "        print(\"Error.\")\n",
    "        print(f\"Modelo esperado:\")\n",
    "        print(\n",
    "            f\"Tarea: {task_number} | Tamaño: {clipping_side_size} | Salto: \"\n",
    "            f\"{clipping_jump_size} | Epochs: {training_epochs} | Sujetos: \"\n",
    "            f\"{train_id_list}\"\n",
    "        )\n",
    "        print(f\"Modelo cargado:\")\n",
    "        print(\n",
    "            f\"Tarea: {model.task_number} | Tamaño: {model.clipping_side_size} | Salto\"\n",
    "            f\": {model.clipping_jump_size} | Epochs: {model.training_epochs} | Sujeto\"\n",
    "            f\"s: {model.train_id_list}\"\n",
    "        )\n",
    "        print(\"Operación abortada.\")\n",
    "        sys.exit()\n",
    "\n",
    "# Si no hay un modelo en la ruta, se entrena uno nuevo.\n",
    "else:\n",
    "    model = nn.ConvolutionalNetwork(\n",
    "        task_number,\n",
    "        clipping_side_size,\n",
    "        clipping_jump_size,\n",
    "        training_epochs,\n",
    "        train_id_list,\n",
    "    )\n",
    "    print(\"Empieza el entrenamiento:\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Se obtiene el modelo y los datos del entrenamiento.\n",
    "    model = nn.train(model, training_epochs, train_loader, test_loader)\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Modelo entrenado en {(end_time - start_time):.2f} segundos.\")\n",
    "\n",
    "    # Se guarda el modelo para no tener que repetir el entrenamiento.\n",
    "    file = open(model_file_path, \"wb\")\n",
    "    pickle.dump(model, file)\n",
    "    file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Se generan las predicciones sobre los sujetos deseados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Una vez entrenado el modelo se puede usar para generar predicciones.\n",
    "# Posibles valores, entre otros: list(subjects_tasks.keys()), [1, 2, 3], subjects_id_list, test_id_list\n",
    "predict_subjects = list(subjects_tasks.keys())\n",
    "\n",
    "# Se descartan los sujetos que no existen y los que se han usado para entrenamiento.\n",
    "target_subjects = []\n",
    "for subject_id in predict_subjects:\n",
    "    if subject_id not in list(subjects_tasks.keys()):\n",
    "        print(f\"El sujeto {subject_id} no existe, descartado.\")\n",
    "\n",
    "    # Comentar si se quiere realizar la predicción sobre algún sujeto de entrenamiento.\n",
    "    elif subject_id in train_id_list:\n",
    "        print(f\"El sujeto {subject_id} se ha usado en el entrenamiento, descartado.\")\n",
    "\n",
    "    # Para la configuración 4, se descartan los valores atípicos.\n",
    "    # elif subject_id in (outliers_false_negatives or outliers_false_positives):\n",
    "    #     print(f\"El sujeto {subject_id} es un valor atípico en la predicción, descartado.\")\n",
    "\n",
    "    else:\n",
    "        target_subjects.append(subject_id)\n",
    "\n",
    "# Se generan los recortes de los sujetos y los diccionarios para realizar la predicción.\n",
    "print(\"Generando los recortes para realizar las predicciones\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Las imágenes de los recortes se guardan en un directorio cuyo nombre depende de la tarea, del tamaño\n",
    "# de salto y del tamaño de recorte. De esta manera pueden ser usadas en ejecuciones posteriores.\n",
    "for subject_id in target_subjects:\n",
    "    for letters_set in subjects_tasks[subject_id][task_number].letters_sets_list:\n",
    "        letters_set.generate_clippings(clipping_side_size, clipping_jump_size)\n",
    "\n",
    "# Las imágenes de los recortes generados se copian al directorio temp para realizar las predicciones.\n",
    "shutil.rmtree(os.path.join(\"generated\", \"temp\"), ignore_errors=True)\n",
    "os.makedirs(os.path.join(\"generated\", \"temp\"))\n",
    "\n",
    "temp_clippings = {}\n",
    "temp_clippings_pd = {}\n",
    "\n",
    "for subject_id in target_subjects:\n",
    "    for letters_set in subjects_tasks[subject_id][task_number].letters_sets_list:\n",
    "        for clipping in letters_set.clippings_list:\n",
    "            temp_clippings[clipping.name] = clipping\n",
    "            temp_clippings_pd[clipping.name] = subjects_pd_status_years[subject_id][0]\n",
    "            clipping.copy_clipping(\"temp\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Recortes generados en {(end_time - start_time):.2f} segundos.\")\n",
    "\n",
    "# Se generan las predicciones.\n",
    "print(\"Generando las predicciones.\")\n",
    "start_time = time.time()\n",
    "\n",
    "temp_dataset = nn.ClippingsDataset(\"temp\", temp_clippings_pd, temp_clippings)\n",
    "nn.predict(model, temp_dataset, clipping_side_size)\n",
    "\n",
    "# Se generan las predicciones a nivel de trazo, conjunto de letras y tarea.\n",
    "for subject_id in target_subjects:\n",
    "    subjects_tasks[subject_id][task_number].generate_prediction_results()\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Predicciones generadas en {(end_time - start_time):.2f} segundos.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Se generan los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por último se genera el plot de las tareas sobre las que\n",
    "# se ha realizado la predicción y se guardan los resultados.\n",
    "shutil.rmtree(os.path.join(\"results\", f\"{task_number}_predicted_by_id\"), ignore_errors=True)\n",
    "os.makedirs(os.path.join(\"results\", f\"{task_number}_predicted_by_id\"))\n",
    "\n",
    "px = 1 / plt.rcParams[\"figure.dpi\"]\n",
    "\n",
    "for subject_id in target_subjects:\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_figwidth(1200 * px)\n",
    "    ax.set_aspect(\"equal\", \"box\")\n",
    "    ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    colors = \"tab:green\", \"tab:red\"\n",
    "    ls_predicted_text = \"H\", \"PD\"\n",
    "    task_predicted_text = \"\\nPredicción: Sano.\", \"\\nPredicción: sufre PD.\"\n",
    "    no_predicted_coordinates = False\n",
    "\n",
    "    for letters_set in subjects_tasks[subject_id][task_number].letters_sets_list:\n",
    "        # Primero se genera el plot de los trazos originales en base a la predicción\n",
    "        # de la letra. De esta manera no quedan huecos entre los nuevos trazos.\n",
    "        for stroke in letters_set.strokes_list:\n",
    "            stroke_x_list = stroke.get_x_coordinates_list()\n",
    "            stroke_y_list = stroke.get_y_coordinates_list()\n",
    "            ax.plot(\n",
    "                stroke_x_list, stroke_y_list, color=colors[letters_set.pd_predicted]\n",
    "            )\n",
    "\n",
    "        # Se superponen los trazos de la predicción.\n",
    "        for stroke in letters_set.predicted_strokes_list:\n",
    "            stroke_x_list = stroke.get_x_coordinates_list()\n",
    "            stroke_y_list = stroke.get_y_coordinates_list()\n",
    "            ax.plot(stroke_x_list, stroke_y_list, color=colors[stroke.pd_predicted])\n",
    "\n",
    "        # Se muestran las coordenadas sin predicción.\n",
    "        for coordinate in letters_set.unpredicted_coordinates_list:\n",
    "            ax.scatter(coordinate[0], coordinate[1], color=\"tab:purple\")\n",
    "\n",
    "        if len(letters_set.unpredicted_coordinates_list) > 0:\n",
    "            no_predicted_coordinates = True\n",
    "\n",
    "        # Se muestra la predicción a nivel de conjunto de letras.\n",
    "        text_coordinates = letters_set.get_text_coordinates()\n",
    "        plt.text(\n",
    "            text_coordinates[0],\n",
    "            text_coordinates[1],\n",
    "            ls_predicted_text[letters_set.pd_predicted],\n",
    "            color=colors[letters_set.pd_predicted],\n",
    "        )\n",
    "\n",
    "    title_string = f\"Tarea {task_number} del sujeto {subject_id} (\"\n",
    "    if subjects_pd_status_years[subject_id][0] == 1:\n",
    "        title_string += (\n",
    "            f\"sufre PD desde hace {subjects_pd_status_years[subject_id][1]} años).\"\n",
    "        )\n",
    "    else:\n",
    "        title_string += \"sano)\"\n",
    "\n",
    "    title_string += task_predicted_text[\n",
    "        subjects_tasks[subject_id][task_number].pd_predicted\n",
    "    ]\n",
    "    plt.suptitle(title_string)\n",
    "\n",
    "    h_line = mlines.Line2D([], [], color=\"tab:green\", label=\"Sano\")\n",
    "    pd_line = mlines.Line2D([], [], color=\"tab:red\", label=\"PD\")\n",
    "    np_point = mlines.Line2D(\n",
    "        [], [], color=\"tab:purple\", marker=\"o\", markersize=10, label=\"Punto sin recorte\"\n",
    "    )\n",
    "\n",
    "    if no_predicted_coordinates:\n",
    "        fig.legend(handles=[h_line, pd_line, np_point], loc=\"upper left\")\n",
    "    else:\n",
    "        fig.legend(handles=[h_line, pd_line], loc=\"upper left\")\n",
    "\n",
    "    plt.savefig(\n",
    "        os.path.join(\"results\", f\"{task_number}_predicted_by_id\", f\"{subject_id}.png\")\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "output_file_path = os.path.join(\"results\", f\"{task_number}_prediction_results.txt\")\n",
    "output_file = open(output_file_path, \"a\", encoding=\"utf-8\")\n",
    "\n",
    "# Generación de la matriz de confusión de conjuntos y tareas.\n",
    "# Corrección, precisión, recall, F1-score\n",
    "real_value = []\n",
    "predicted_value = []\n",
    "\n",
    "for subject_id in target_subjects:\n",
    "    if subjects_pd_status_years[subject_id][0] == 0:\n",
    "        real_value.append(\"H\")\n",
    "    else:\n",
    "        real_value.append(\"PD\")\n",
    "    if subjects_tasks[subject_id][task_number].pd_predicted == 0:\n",
    "        predicted_value.append(\"H\")\n",
    "    else:\n",
    "        predicted_value.append(\"PD\")\n",
    "\n",
    "real_value = np.array(real_value)\n",
    "predicted_value = np.array(predicted_value)\n",
    "\n",
    "c_matrix = confusion_matrix(real_value, predicted_value)\n",
    "\n",
    "sns.heatmap(c_matrix, annot=True, fmt=\"g\", xticklabels=[\"H\", \"PD\"], yticklabels=[\"H\", \"PD\"])  # type: ignore\n",
    "plt.ylabel(\"Predicción\", fontsize=13)\n",
    "plt.xlabel(\"Valor real\", fontsize=13)\n",
    "plt.title(\"Matriz de confusión (tareas)\", fontsize=17)\n",
    "\n",
    "plt.savefig(os.path.join(\"results\", f\"{task_number}_tasks_confusion_matrix.png\"))\n",
    "plt.close()\n",
    "\n",
    "output_file.write(\n",
    "    f\"Semilla: {random_seed} | Tamaño: {clipping_side_size}\\nTareas: Corrección: \"\n",
    "    f\"{accuracy_score(real_value, predicted_value)*100:.2f}% | Precisión: \"\n",
    "    f\"{precision_score(real_value, predicted_value, pos_label='PD')*100:.2f}% | Recall: \"\n",
    "    f\"{recall_score(real_value, predicted_value, pos_label='PD')*100:.2f}% | \"\n",
    "    f\"F1-score: {f1_score(real_value, predicted_value, pos_label='PD')*100:.2f}%\\n\"\n",
    ")\n",
    "\n",
    "real_value = []\n",
    "predicted_value = []\n",
    "\n",
    "for subject_id in target_subjects:\n",
    "    for letters_set in subjects_tasks[subject_id][task_number].letters_sets_list:\n",
    "        if subjects_pd_status_years[subject_id][0] == 0:\n",
    "            real_value.append(\"H\")\n",
    "        else:\n",
    "            real_value.append(\"PD\")\n",
    "        if letters_set.pd_predicted == 0:\n",
    "            predicted_value.append(\"H\")\n",
    "        else:\n",
    "            predicted_value.append(\"PD\")\n",
    "\n",
    "real_value = np.array(real_value)\n",
    "predicted_value = np.array(predicted_value)\n",
    "\n",
    "c_matrix = confusion_matrix(real_value, predicted_value)\n",
    "\n",
    "sns.heatmap(c_matrix, annot=True, fmt=\"g\", xticklabels=[\"H\", \"PD\"], yticklabels=[\"H\", \"PD\"])  # type: ignore\n",
    "plt.ylabel(\"Predicción\", fontsize=13)\n",
    "plt.xlabel(\"Valor real\", fontsize=13)\n",
    "plt.title(\"Matriz de confusión (letras)\", fontsize=17)\n",
    "\n",
    "plt.savefig(os.path.join(\"results\", f\"{task_number}_letters_confusion_matrix.png\"))\n",
    "plt.close()\n",
    "\n",
    "output_file.write(\n",
    "    f\"Conjuntos: Corrección: {accuracy_score(real_value, predicted_value)*100:.2f}% | \"\n",
    "    f\"Precisión: {precision_score(real_value, predicted_value, pos_label='PD')*100:.2f}% | \"\n",
    "    f\"Recall: {recall_score(real_value, predicted_value, pos_label='PD')*100:.2f}% | \"\n",
    "    f\"F1-score: {f1_score(real_value, predicted_value, pos_label='PD')*100:.2f}%\\n\"\n",
    ")\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# Colab. Para poder descargar los resultados.\n",
    "shutil.make_archive(\"results\", \"zip\", \"results\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
